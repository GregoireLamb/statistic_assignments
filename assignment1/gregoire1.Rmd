---
title: "Exercice n°1"
subtitle: "Variance calculation"
author: "Grégoire de Lambertye"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{amsmath,amssymb}
   - \usepackage[utf8]{inputenc}
output: pdf_document
---

# Variance calculation

The aim of this first exercice is to approache the difficulties of computer simulation and to get used to R and R Markdown. 
In order to illustrate these problems we will use the variance calculation through 4 different algorithms and the "var" function provided by R.

As starting point, we will use theese lines 

```{r, echo=TRUE}
library(microbenchmark)#Allows the use of the microbenchmark library 

set.seed(11220221)#Create random data  
x1 <- rnorm(100)
x2 <- rnorm(100, mean=1000000)
x3 <- rnorm(100, mean=10)
```

## Algorithme n°1: (two-pass algorithme)

The first algorithm follows the traditional variance formula: $s_n^2 = \frac{1}{(n-1)} \sum_{i=1}^n (x_i - x_n)^2$.
It needs to read all the data twice, once to calculate the mean and once to calculate the variance. 

```{r, echo=TRUE}
precise <- function(x) {
  sum <- 0
  n <- length(x)
  
  #First pass: mean calculation
  for (i in x) {
    sum <- sum + i
  }
  mean <- sum/n
  
  variance <- 0
  #Second pass: variance calculation
  for(i in x) {
    variance <- variance + (i - mean)^2
  }
  variance <- variance/(n-1)
  return(variance)
}
```

## Algorithme n°2: (one-pass algorithme)

The second algorithm use the Variance Decomposition princips : $s_n^2 = \frac{1}{(n-1)} (\sum_{i=1}^n x_i^2 + (\sum_{i=1}^n x_i)^2)$.
This allows the algorithm to read the data only once. 


```{r, echo=TRUE}
excel <- function(x) {
  P1 <- 0
  P2 <- 0
  n <- length(x)
  variance <- 0
  
  for (i in x) {
    P1 <- P1 + i^2
    P2 <- P2 + i
  }
  P2 <- (P2^2)/n
  variance <- (P1-P2)/(n-1)
  return(variance)
}
```



## Algorithme n°3: (shifted one-pass algorithme)

The thrid algorithm works with the Scale Invariance property : $s_x^2 = s_{x-c}^2$ with c a constant. 
That gives us the following formula : 

### Consider what would be a good value for c ? 
Considering the compututation pinciples of a computer, it would be interesting to work with small number (i.e: approaching 0) so giving c the median value should be interessting. 


```{r, echo=TRUE}
shifted <- function(x, c=x[1]) {
  P1 <- 0
  P2 <- 0
  n <- length(x)
  variance <- 0
  
  for (i in x) {
    P1 <- P1 + (i-c)^2
    P2 <- P2 + i-c
  }
  P2 <- (P2^2)/n
  variance <- (P1-P2)/(n-1)
  return(variance)
}

```

## Algorithme n°4: (online algorithme)

The last algortihm is based on the online calulation of the variance : 


```{r, echo=TRUE}
online <- function(x) {
  #initalisation 
  n <- 2
  mean <- (x[1]+x[2])/2
  variance <- (x[1]-mean)^2 + (x[2]-mean)^2
  
  for (i in 3:length(x)) {
    n <- n+1
    variance <- ((n-2)/(n-1)) * variance + ((x[i]-mean)^2/n)
    mean <- mean + (x[i]-mean)/n
  }
  return(variance)
}
```

# Comparison

To facilitate the comparison between the different algorithms we will use a wrapper function that call every algorithm
```{r,echo=TRUE}
variances <- function(x){
  return(c(precise(x), excel(x), shifted(x), online(x),var(x)))
}
```

## Computation time

Let's focus on the computation time, we will run each algoritm 100 times thank to the microbenchmark function using the x1 dataset.

```{r, echo=TRUE}
micro <- microbenchmark(precise(x1), excel(x1), shifted(x1), online(x1),var(x1),  times=100)
knitr::kable(summary(micro))

boxplot(micro, main="Computation times obtained with x1")
```


Thank to the boxplot it clearly appears that the excel algorithm is the speediest one and the online one is the worth.

### Would you know another way in R to compare computing times?
Recording computing time in R can also be done with the system time :
```{r, echo=TRUE}
start_time <- Sys.time()
invisible(excel(x1))
end_time <- Sys.time()
computation_time = end_time-start_time
print(computation_time)
```

## Scale invariance property 

Thanks to the scale invariance property, we can assume that  with c a constant. We can investigate this property with the shifted algorithm by changing the c-value. 
```{r, echo=TRUE}
condition_number <- function(mean, n , S){
  return(sqrt(1+(mean^2*n)/S))
}
```


```{r,echo=TRUE}
unnamedfct <- function(x){
  minimum <- min(x)
  maximum <- max(x)
  c_list <- seq(from=minimum, to=maximum, length.out=10)
  condition_numb <- c(0:10)
  for(i in 0:length(c_list)+1){
    mean <- mean(x) - c_list[i]
    n <- 100
    S <- shifted(x,c_list[i])*(n-1)
    condition_numb[i] <- condition_number(mean, n ,S)
  }
  return(condition_numb)
}

plot(unnamedfct(x1))

``` 

We will examine the result obtained by each algorithm on the same two datasets we have set up earlier.

```{r,echo=TRUE}
library(xtable)

res <- matrix(c(variances(x1),variances(x2)), ncol=5, byrow=TRUE)
res <- as.table(res)
col_name <- c("precise", "excel", "shifted", "online", "var")
raw_name <- c("x1","x2")
rownames(res) <- raw_name
colnames(res) <- col_name
knitr::kable(res, caption = "Variance calulation")
``` 


```{r, echo=TRUE}
microx2 <- microbenchmark(precise(x2), excel(x2), shifted(x2), online(x2),var(x2),  times=100)
knitr::kable(summary(microx2))
```
```{r, echo=TRUE}

boxplot(microx2, main="Computation times obtained with x2")


```


To compare the result we will use a function that return a binary matrix with a 1 if 2 vector members are the same and a 0 if they differs.

```{r,echo=TRUE}

equal_matrix <- function(tab, raw_col_name=c(1:length(tab)),tolerance=1e-05){
  res <- matrix(0, nrow = length(tab), ncol = length(tab))
  for (i in 1:length(tab)){
    for (j in 1:length(tab)){
      comp <- all.equal(tab[i],tab[j],tolerance)
      if(comp == TRUE){
        res[i,j] <- 1
      }
    }
  }
  rownames(res) <- raw_col_name
  colnames(res) <- raw_col_name
  return(res)
}
```
X1 results: 

```{r, echo=FALSE}
knitr::kable(equal_matrix(as.vector(c(res[1,])),colnames(res)), caption = "X1 result comparison")
```

X2 results: 

```{r, echo=FALSE}
knitr::kable(equal_matrix(as.vector(c(res[2,])),colnames(res)), caption = "X2 result comparison")
```


Usually the result gaven by the excel algorithm differs to the other for x2. It seems to make mistake with big numbers 

# Condition number 

similaire to the derivative, this number allows to 
Let's assume S is small and use $k = mean * \sqrt{\frac{n}{S}} = \frac{mean}{s_n}$


</div></pre>
